# Run files for setting new SOTA for MiniLM

The difference withing these run files is that we do not truncate the documents similar to the existing effective MiniLM reranker [ms-marco-MiniLM-L-12-v2](https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2) and use the exact hyperparameters by them to do training.

The notebooks for training these run files are as below:
  1. For [KD-MiniLM_BM25CAT](https://colab.research.google.com/drive/1mzWJ3vBciCYpjce75rHirLwUYL_4nTdS?usp=sharing) [![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1mzWJ3vBciCYpjce75rHirLwUYL_4nTdS?usp=sharing)
  2. For [KD-MiniLM_DPRCAT]()
  3. For [KD-MiniLM_DPRBM25CAT]()

The evaluation notebook for evaluating each of run files: (Comming soon)

The evaluation results for each run files: (Comming soon)
